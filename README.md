# ExplainableAI
Project on Post Hoc interpretability

As modern Artificial Intelligence is being integrated into critical industries like healthcare, self-driving cars, and finance, it is very important to understand how machine learning and deep learning models actually predict before trusting their results. For example, in a financial company, we can't rely on a model just because it claims to detect 100% of fraudulent transactions. Similarly, we can't deploy a self-driving taxi service just because the model performs well during testing. In the medical field, we shouldn't use an AI model in healthcare simply because it gives good results on test data.

The main goal of any AI researcher should be to understand why a model is making certain predictions, regardless of the outcomes. It's important to know what factors or features of the input datasets  are influencing the model's decisions. Researchers should cross verify if the model is truly learning patterns from the data, or is it just making random guesses? A model should only be deployed in the real world when researchers fully understand how it makes decisions and why it gives certain results. This helps ensure the system is safe, fair, and reliable and will not have negative consequences for living beings.

There are several ways to help humans understand the black box nature of deep learning and machine learning models. These methods explain why a model makes a specific prediction. Post-hoc interpretability is one of the methods which can be applied to models after completion of the training. In post hoc interpretability there are two subfields which are model agnostic and model specific. Model agnostic methods are further divided into local methods and global methods. Local methods help us to understand the individual predictions whereas the Global methods tell us the overall prediction.

For the coursework, a deep learning model was developed to classify and identify images of snakes, dogs, and cats.For experimentation, the project implemented three convolutional blocks, each containing two convolutional layers followed by a max pooling layer to reduce the dimensions and size of the image. ReLU was used as the activation function after each convolution, and dropout with a rate of 0.5 was applied before the final fully connected layer for regularization. After training, we will try to analyse and understand why the model predicts a particular class and what factors influence its decision. To do this, we implemented two methods, which are LIME, which is a model-agnostic technique, and Grad-CAM, which is a model-specific method and these techniques were applied to our trained model to interpret its predictions. 

